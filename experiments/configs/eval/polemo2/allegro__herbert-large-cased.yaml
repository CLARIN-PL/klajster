accelerator: auto
config: !!python/object:embeddings.config.lightning_config.LightningAdvancedConfig
  batch_encoding_kwargs: {}
  dataloader_kwargs: {}
  datamodule_kwargs:
    max_seq_length: 512
  early_stopping_kwargs:
    mode: min
    monitor: val/Loss
    patience: 3
  finetune_last_n_layers: 4
  model_config_kwargs:
    classifier_dropout: 0.0
  task_model_kwargs:
    adam_epsilon: 0.0001
    eval_batch_size: 32
    learning_rate: 0.0001
    optimizer: AdamW
    train_batch_size: 32
    use_scheduler: true
    warmup_steps: 200
    weight_decay: 0.01
  task_train_kwargs:
    max_epochs: 3
    profiler: pytorch
  tokenizer_kwargs: {}
dataset_name_or_path: data/datasets/polemo2/lightning_hps/
devices: gpu
embedding_name_or_path: allegro/herbert-large-cased
input_column_name: text
load_dataset_kwargs: null
predict_subset: !!python/object/apply:embeddings.data.dataset.LightingDataModuleSubset
- test
target_column_name: target
tokenizer_name_or_path: allegro/herbert-large-cased
